# -*- coding: utf-8 -*-
"""PartB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zSVEWCiQIj_FcWBHoFCCO3RbnIjkz1Es
"""

# Commented out IPython magic to ensure Python compatibility.
# Jupyter-specific.
# %matplotlib inline
import time
program_start = time.time()

# Colab specific.
from google.colab import files

# SciKit Learn
from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import average_precision_score
from sklearn.metrics import recall_score
from sklearn.model_selection import KFold
from sklearn import neighbors
from sklearn import tree
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, ensemble
from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics
from sklearn import preprocessing
from sklearn.model_selection import GridSearchCV

# Keras
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.layers import Embedding
from keras.layers import LSTM, GRU, CuDNNGRU, CuDNNLSTM
from keras.layers import Conv1D, MaxPooling1D
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM
from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint
from keras.layers import GlobalMaxPooling1D

# General 
from collections import Counter
import numpy as np
import pandas as pd
import nltk
from nltk.corpus import stopwords
# Might need to uncomment the line below to downlaod nltk stopwords
nltk.download('stopwords')
stop = stopwords.words('english')
import matplotlib.pyplot as plt
import seaborn as sns; sns.set() # Set the default styling. 
from io import BytesIO
import csv
import pickle
from scipy.sparse import coo_matrix, vstack, csr_matrix
import re
import warnings
warnings.filterwarnings('ignore')

# Plot style settings.
plt.style.use('fivethirtyeight') # I'm a fan of this one.

"""# **Reduced Data Work**

### Working with the reduced data

Reading the reduced file, taking the review and rating column
"""

y = []
X = []
# open reduced CSV
with open('reduced_amazon_ff_reviews.csv', 'r') as csvFile:
    reader = csv.reader(csvFile)
    for row in reader:
        X.append(row[5])
        y.append(row[8])

"""Making a data frame called 'df' out of column 5 and 8"""

# columns 'Rating' and 'Text' are loaded into the df
df = pd.read_csv('reduced_amazon_ff_reviews.csv', usecols=[5, 8])

# seeing the top 5 records
df.head()

"""### Setting up the data for LSTM (reduced)

Initalizing the variables for the LSTM Model. Numbers taken from the docs located [here](https://keras.io/examples/imdb_cnn_lstm/)
"""

# For Embedding
max_features = 20000
maxlen = 100
embedding_size = 128

# Convolution
kernel_size = 5
filters = 64
pool_size = 4

# LSTM
lstm_output_size = 70

# Training
batch_size = 30
epochs = 2

"""Getting the data ready for emebdding, filtered out neutural reviews, number, speical characters, stopwords and converted to lowercase for better reading"""

# Removing Neutral 
df = df[df.Rating != "neutral"]

# Converting to lowercase
df['Text'] = df['Text'].apply(lambda x: str(x).lower())

# Removing numbers/special characters
df['Text'] = df['Text'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))

"""Checking to make sure the filters worked"""

df.head()

"""Printing the number of positive and negative reviews (reduced set)"""

# Number of positive
print(df[ df['Rating'] == 'positive'].size)

# Number of negative
print(df[ df['Rating'] == 'negative'].size)

"""Toeknizing the data for word embeddings. Lossely modeled after the solutions in this [link](https://stackoverflow.com/questions/55578939/tokenize-each-row-in-a-dataframe-for-loop-not-working)"""

for idx,row in df.iterrows():
    row[0] = row[0].replace('rt',' ')
    
tokenizer = Tokenizer(nb_words = max_features, split=' ')
tokenizer.fit_on_texts(df['Text'].values)
X = tokenizer.texts_to_sequences(df['Text'].values)
X = pad_sequences(X, maxlen = maxlen)

"""Using get_dummies to convert the categorical text data into indicator variables. Based off the documentation [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)"""

# Using the get_dummies function on the data frame
Y = pd.get_dummies(df['Rating']).values

# Split the data 80/20 and add random state 42
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)

# Print the shapes of the training and testing set
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

"""### LSTM Model and Results

Building the First LSTM Model. It's a sequential model with 6 layers
"""

def lstmReduced1():
# Inform that the model is building
  print('Build model...')

  model = Sequential()

  # Add the embedding layer
  model.add(Embedding(max_features, embedding_size, input_length=maxlen))
  model.add(Dropout(0.25))
  # 1D Convolution Layer with Relu activation
  model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))
  model.add(MaxPooling1D(pool_size=pool_size))

  # Add LSTM Layer
  model.add(LSTM(lstm_output_size))

  # Use the Softmask activiation function
  model.add(Dense(2,activation='softmax'))

  # Using categorical corss entropy and the adam optimizer
  model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])

  # Print a summar of the model
  print(model.summary())

  return model

"""Calling the lstmReduced1 function to the variable LSTM1"""

LSTM1 = lstmReduced1()

"""Training the LSTM1 Model with a Batch Size of 30 and 2 Epochs"""

print('Train...')
LSTM1.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,shuffle=True, validation_data=(X_test, Y_test))
print('Model evaluation ',LSTM1.evaluate(X_test,Y_test))

"""Printing the test score and test accuracy"""

score, acc = LSTM1.evaluate(X_test, Y_test, batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)

"""Using the classification_report function from Scikit learn to return the main classification metrics of the model (precision, recall, f1-score, support) Based off the documentation found [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"""

LSTM1_Metrics = LSTM1.predict(X_test, verbose=2, batch_size=batch_size)
print(metrics.classification_report(Y_test[:,1], np.round(LSTM1_Metrics[:,1]), target_names =["Negative", "Positive"]))

"""### Experimenting with Different LSTM Epochs (reduced)

Increasing the number of Epochs to find when the data starts to overfit
"""

# Training with 17 more epochs
print('Train...')
historyLSTM1 = LSTM1.fit(X_train, Y_train, batch_size=batch_size, epochs=20,shuffle=True, validation_data=(X_test, Y_test))
print('Model evaluation ', LSTM1.evaluate(X_test,Y_test))

# Printing test score and accuracy with 20 epochs
score, acc = LSTM1.evaluate(X_test, Y_test, batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)

"""Function that plots the loss versus epoch number. Code taken from the tutorial provided in the project module, found [here](https://realpython.com/python-keras-text-classification/)"""

def plot_history(history):
    acc = history.history['acc']
    val_acc = history.history['val_acc']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='Training acc')
    plt.plot(x, val_acc, 'r', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='Training loss')
    plt.plot(x, val_loss, 'r', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()

"""Calling the plot function"""

plot_history(historyLSTM1)

"""According to the RealPython tutorial in the link above, a good way to see when the model starts overfitting is when the loss of validation starts rising again. This is around Epoch 5-6 based on the graph. So let's train the model with 6 epochs and see how it comapres to the one with 2"""

# Training with 3 epochs
print('Train...')
historyLSTM1 = LSTM1.fit(X_train, Y_train, batch_size=batch_size, epochs=6,shuffle=True, validation_data=(X_test, Y_test))
print('Model evaluation ', LSTM1.evaluate(X_test,Y_test))

# Printing test score and accuracy with 3 epochs
score, acc = LSTM1.evaluate(X_test, Y_test, batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)

"""With 6 epochs we got a slightly higher accuracy without overfitting"""

LSTM1_Metrics = LSTM1.predict(X_test, verbose=2, batch_size=batch_size)
print(metrics.classification_report(Y_test[:,1], np.round(LSTM1_Metrics[:,1]), target_names =["Negative", "Positive"]))

"""### Experiementing with other LSTM models (reduced)

Now that we know the 'best' number of epochs for the reduced LSTM set, let's make a few more models and change some things to see if we can get better results

Changing the Conv1D layer to have a sigmoid activation and the density layer to have the softplus activation function
"""

def lstmReduced2():
# Inform that the model is building
  print('Build model...')

  model = Sequential()

  # Add the embedding layer
  model.add(Embedding(max_features, embedding_size, input_length=maxlen))
  model.add(Dropout(0.25))
  # 1D Convolution Layer with Sigmoid activation
  model.add(Conv1D(filters, kernel_size, padding='valid', activation='sigmoid', strides=1))
  model.add(MaxPooling1D(pool_size=pool_size))

  # Add LSTM Layer
  model.add(LSTM(lstm_output_size))

  # Use the Softplus activiation function
  model.add(Dense(2,activation='softplus'))

  # Using categorical corss entropy and the SGD optimizer
  model.compile(loss = 'categorical_crossentropy', optimizer='SGD',metrics = ['accuracy'])

  # Print a summar of the model
  print(model.summary())

  return model

"""Building the second LSTM model"""

LSTM2 = lstmReduced2()

"""Training the second model with 6 epochs"""

print('Train...')
LSTM2.fit(X_train, Y_train, batch_size=batch_size, epochs=6,shuffle=True, validation_data=(X_test, Y_test))
print('Model evaluation ',LSTM2.evaluate(X_test,Y_test))

"""Printing the results of model 2"""

# Printing test score and accuracy of the second LSTM model
score, acc = LSTM2.evaluate(X_test, Y_test, batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)

# Getting the classification report of LSTM model 2
LSTM2_Metrics = LSTM2.predict(X_test, verbose=2, batch_size=batch_size)
print(metrics.classification_report(Y_test[:,1], np.round(LSTM2_Metrics[:,1]), target_names =["Negative", "Positive"]))

"""The above results for model 2 were lacking compared to the first LSTM. Let's make another model to see if we can improve it

Changing the Conv1D layer to have a linear activation and the density layer to have the softsign activation function. Also changed the optimizer to Adadelta
"""

def lstmReduced3():
# Inform that the model is building
  print('Build model...')

  model = Sequential()

  # Add the embedding layer
  model.add(Embedding(max_features, embedding_size, input_length=maxlen))
  model.add(Dropout(0.25))
  # 1D Convolution Layer with Relu activation
  model.add(Conv1D(filters, kernel_size, padding='valid', activation='linear', strides=1))
  model.add(MaxPooling1D(pool_size=pool_size))

  # Add LSTM Layer
  model.add(LSTM(lstm_output_size))

  # Use the Softsign activiation function
  model.add(Dense(2,activation='softsign'))

  # Using categorical corss entropy and the Adadelta optimizer
  model.compile(loss = 'categorical_crossentropy', optimizer='Adadelta',metrics = ['accuracy'])

  # Print a summar of the model
  print(model.summary())

  return model

"""Building the model"""

LSTM3 = lstmReduced3()

"""Train the third LSTM model with 6 epochs"""

print('Train...')
LSTM3.fit(X_train, Y_train, batch_size=batch_size, epochs=6,shuffle=True, validation_data=(X_test, Y_test))
print('Model evaluation ',LSTM3.evaluate(X_test,Y_test))

"""Printing the results of LSTM model 3"""

# Printing test score and accuracy of the third LSTM model
score, acc = LSTM3.evaluate(X_test, Y_test, batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)

"""Has a worse Accuracy the LSTM Reduced 2. So based on our expeirments on the reduced set. **LSTM Model 1 has the best results of the reduced set.** Expeirments with the full data set are in the Section "Full Data Work"

### Setting up the Data for CNN (Reduced)

Initalizing the variables for the CNN model. Numbers taken from [here](https://keras.io/examples/imdb_cnn/)
"""

# Note some parameters were changed to be more consistent with the LSTM 
# experiements so we could have an accurate comaprison
max_features = 20000
maxlen = 100
batch_size = 30
embedding_dims = 50
filters = 250
kernel_size = 3
hidden_dims = 250
epochs = 2

"""### CNN Model and Results

Creating the function for the first CNN model. Based off the link above. It is a sequential model with 9 layers. The code section below this also trains the model on 2 epochs
"""

def cnnReduced1():
  print('Build model...')
  model = Sequential()

  # we start off with an efficient embedding layer which maps
  # our vocab indices into embedding_dims dimensions
  model.add(Embedding(max_features,
                      embedding_dims,
                      input_length=maxlen))
  model.add(Dropout(0.2))

  # we add a Convolution1D, which will learn filters
  # word group filters of size filter_length:
  model.add(Conv1D(filters,
                  kernel_size,
                  padding='valid',
                  activation='relu',
                  strides=1))
  # we use max pooling:
  model.add(GlobalMaxPooling1D())

  # We add a vanilla hidden layer:
  model.add(Dense(hidden_dims))
  model.add(Dropout(0.2))
  model.add(Activation('relu'))

  # We project onto a single unit output layer, and squash it with a sigmoid:
  model.add(Dense(2))
  model.add(Activation('sigmoid'))

  model.compile(loss='categorical_crossentropy',
                optimizer='adam',
                metrics=['accuracy'])
  
  print(model.summary())
  
  return model

"""Building the model"""

CNN1 = cnnReduced1()

"""Training the model"""

print('Train...')
CNN1.fit(X_train, Y_train, batch_size=batch_size, epochs=2,shuffle=True, validation_data=(X_test, Y_test))
print('Model evaluation ',CNN1.evaluate(X_test,Y_test))

"""Printing the results of the first CNN model (reduced)"""

# Printing the accuracy and test score of the first CNN1 model
score, acc = CNN1.evaluate(X_test, Y_test, batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)

# Getting the classification metrics of CNN1 model 
CNN1_Metrics = CNN1.predict(X_test, verbose = 2, batch_size = batch_size)
print(metrics.classification_report(Y_test[:,1], np.round(CNN1_Metrics[:,1]) ,target_names = ["Negative", "Positive"]))

"""### Experiementing with Different CNN Epochs (reduced)

Like with our LSTM data, we're going to increase the number of Epochs so we can see where the data starts to overfit. To keep it a fair comparison, we will train our first model on 20 epochs and see if it has a different point of overfitting compared to LSTM
"""

# Training with 17 more epochs
print('Train...')
historyCNN1 = CNN1.fit(X_train, Y_train, batch_size=batch_size, epochs=20,shuffle=True, validation_data=(X_test, Y_test))
print('Model evaluation ', CNN1.evaluate(X_test,Y_test))

# Printing test score and accuracy with 20 epochs
score, acc = CNN1.evaluate(X_test, Y_test, batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)

"""Calling the plot function"""

plot_history(historyCNN1)

"""The graph above provides more interesting results compared to the LSTM one. Since we're looking for when "the validation loss starts rising again" I think around the 4-5 epoch range is the right spot. Let's test with 5 and see how it compares to the 2"""

# Training with 3 more epochs
print('Train...')
historyCNN1 = CNN1.fit(X_train, Y_train, batch_size=batch_size, epochs=5,shuffle=True, validation_data=(X_test, Y_test))
print('Model evaluation ', CNN1.evaluate(X_test,Y_test))

# Printing test score and accuracy with 5 epochs
score, acc = CNN1.evaluate(X_test, Y_test, batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)

"""Getting Classification Metrics with 5 Epochs for CNN"""

# Getting the classification metrics of CNN1 model 
CNN1_Metrics = CNN1.predict(X_test, verbose = 2, batch_size = batch_size)
print(metrics.classification_report(Y_test[:,1], np.round(CNN1_Metrics[:,1]) ,target_names = ["Negative", "Positive"]))

"""The results above with 5 epochs are more higher in both accuracy, precision, and recall comapred to the CNN with 2. The increase in numbers is a bit more drastic compared to LSTM. Let's run some experiements with CNN now that we discovered the 'right' number of epochs (5)

### Experimenting with other CNN Models (reduced)

Now that we know the 'best' number of epochs for the reduced CNN set, let's make a few more models and change some things to see if we can get better results.

To get a more accurate comparison between the two models (LSTM and CNN), We will run the same experiements on both.

Making the CNN model 2 and replacing relu with sigmoid, adding the softplus activation function, and changing the optimizer to SGD.
"""

def cnnReduced2():
  print('Build model...')
  model = Sequential()

  # we start off with an efficient embedding layer which maps
  # our vocab indices into embedding_dims dimensions
  model.add(Embedding(max_features,
                      embedding_dims,
                      input_length=maxlen))
  model.add(Dropout(0.2))

  # we add a Convolution1D, which will learn filters
  # word group filters of size filter_length:
  model.add(Conv1D(filters,
                  kernel_size,
                  padding='valid',
                  activation='sigmoid',
                  strides=1))
  # we use max pooling:
  model.add(GlobalMaxPooling1D())

  # We add a vanilla hidden layer:
  model.add(Dense(hidden_dims))
  model.add(Dropout(0.2))
  model.add(Activation('sigmoid'))

  # We project onto a single unit output layer, and squash it with a sigmoid:
  model.add(Dense(2))
  model.add(Activation('softplus'))

  model.compile(loss='categorical_crossentropy',
                optimizer='SGD',
                metrics=['accuracy'])
  
  print(model.summary())
  
  return model

"""Building the model"""

CNN2 = cnnReduced2()

"""Training our model with 5 epochs"""

print('Train...')
CNN2.fit(X_train, Y_train, batch_size=batch_size, epochs=5,shuffle=True, validation_data=(X_test, Y_test))
print('Model evaluation ',CNN2.evaluate(X_test,Y_test))

"""Printing the results of our second CNN model"""

# Printing the accuracy and test score of the second CNN model
score, acc = CNN2.evaluate(X_test, Y_test, batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)

# Getting the classification metrics of CNN1 model 
CNN2_Metrics = CNN2.predict(X_test, verbose = 2, batch_size = batch_size)
print(metrics.classification_report(Y_test[:,1], np.round(CNN2_Metrics[:,1]) ,target_names = ["Negative", "Positive"]))

"""Like LSTM. The changes to the relu activation function and Adam Optimizer produce much worse results. Let's make another model

Making another model, changing the sigmoid form CNN2 to linear and the optimizer to Adadelta
"""

def cnnReduced3():
  print('Build model...')
  model = Sequential()

  # we start off with an efficient embedding layer which maps
  # our vocab indices into embedding_dims dimensions
  model.add(Embedding(max_features,
                      embedding_dims,
                      input_length=maxlen))
  model.add(Dropout(0.2))

  # we add a Convolution1D, which will learn filters
  # word group filters of size filter_length:
  model.add(Conv1D(filters,
                  kernel_size,
                  padding='valid',
                  activation='linear',
                  strides=1))
  # we use max pooling:
  model.add(GlobalMaxPooling1D())

  # We add a vanilla hidden layer:
  model.add(Dense(hidden_dims))
  model.add(Dropout(0.2))
  model.add(Activation('linear'))

  # We project onto a single unit output layer, and squash it with a sigmoid:
  model.add(Dense(2))
  model.add(Activation('softsign'))

  model.compile(loss='categorical_crossentropy',
                optimizer='Adadelta',
                metrics=['accuracy'])
  
  print(model.summary())
  
  return model

"""Builiding the model"""

CNN3 = cnnReduced3()

"""Training the model with 5 epochs"""

print('Train...')
CNN3.fit(X_train, Y_train, batch_size=batch_size, epochs=5,shuffle=True, validation_data=(X_test, Y_test))
print('Model evaluation ',CNN3.evaluate(X_test,Y_test))

"""Getting the result of CNN 3"""

# Printing the accuracy and test score of the third CNN model
score, acc = CNN2.evaluate(X_test, Y_test, batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)

"""The accuracy wasn't as bad compared to LSTM3, but it was not as high as CNN1. For the reduced CNN data, CNN1 still provides the best results

#**Full Data Work**

### Working with the full data

Reading the full data set and using the review and rating column
"""

y = []
X = []
# open reduced CSV
with open('full_amazon_ff_reviews.csv', 'r') as csvFile:
    reader = csv.reader(csvFile)
    for row in reader:
        X.append(row[5])
        y.append(row[8])

"""Making a data frame called 'dfFull' that is using columns 5 and 8 of the full data"""

# columns 'Rating' and 'Text' are loaded into the df
dfFull = pd.read_csv('full_amazon_ff_reviews.csv', usecols=[5, 8])

# seeing the top 5 records
dfFull.head()

"""### Setting up the Data for LTSM (full)

Initalizing the variables for the LSTM Model. Numbers taken from the docs located [here](https://keras.io/examples/imdb_cnn_lstm/)
"""

# For Embedding
max_features = 20000
maxlen = 100
embedding_size = 128

# Convolution
kernel_size = 5
filters = 64
pool_size = 4

# LSTM
lstm_output_size = 70

# Training
batch_size = 30
epochs = 2

"""Getting the data ready for emebdding, filtered out neutural reviews, number, speical characters, stopwords and converted to lowercase for better reading. Similar to the beginning of the notebook, except this time it's for the data frame containing the full data"""

# Filtering out neutral
dfFull = dfFull[dfFull.Rating != "neutral"]

# Converting all the lowercase
dfFull['Text'] = dfFull['Text'].apply(lambda x: str(x).lower())

# Removing numbers and special cases
dfFull['Text'] = dfFull['Text'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))

"""Counting the number of positive and negative reviews in the full data"""

# Number of positive reviews
print(dfFull[ dfFull['Rating'] == 'positive'].size)

# Number of negative reviews
print(dfFull[ dfFull['Rating'] == 'negative'].size)

"""Toeknizing the data for word embeddings with the full data. Lossely modeled after the solutions in this [link](https://stackoverflow.com/questions/55578939/tokenize-each-row-in-a-dataframe-for-loop-not-working)"""

for idx,row in dfFull.iterrows():
    row[0] = row[0].replace('rt',' ')
    
tokenizer = Tokenizer(nb_words = max_features, split=' ')
tokenizer.fit_on_texts(dfFull['Text'].values)
X = tokenizer.texts_to_sequences(dfFull['Text'].values)
X = pad_sequences(X, maxlen = maxlen)

"""Using get_dummies to convert the categorical text data into indicator variables. Based off the documentation [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)"""

# Call the get_dummies function
Y = pd.get_dummies(dfFull['Rating']).values

# Performing an 80/20 split with a random state of 42
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)

# Printing the shape of both the training and testing data
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

"""### LSTM Model and Results for the Full Data

Building the First LSTM Full Model. It's a sequential model with 6 layers
"""

def lstmFull1():
# Inform that the model is building
  print('Build model...')

  model = Sequential()

  # Add the embedding layer
  model.add(Embedding(max_features, embedding_size, input_length=maxlen))
  model.add(Dropout(0.25))
  # 1D Convolution Layer with Relu activation
  model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))
  model.add(MaxPooling1D(pool_size=pool_size))

  # Add LSTM Layer
  model.add(LSTM(lstm_output_size))

  # Use the Softmask activiation function
  model.add(Dense(2,activation='softmax'))

  # Using categorical corss entropy and the adam optimizer
  model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])

  # Print a summar of the model
  print(model.summary())

  return model

"""Call the lstmFull1 function to the variable LSTM_Full1"""

LSTM_Full1 = lstmFull1()

"""Training the LSTM_Full1 model with a Batch size of 30 and 2 Epochs"""

print('Train...')
LSTM_Full1.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,shuffle=True, validation_data=(X_test, Y_test))
print('Model evaluation ',LSTM_Full1.evaluate(X_test,Y_test))

"""Printing test score and test accuracy. Along with the classification report."""

score, acc = LSTM_Full1.evaluate(X_test, Y_test, batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)

LSTM_Full1_Metrics = LSTM_Full1.predict(X_test, verbose=2, batch_size =batch_size)
print(metrics.classification_report(Y_test[:,1], np.round(LSTM_Full1_Metrics[:,1]), target_names = ["Negative", "Positive"]))

"""### Experimenting with Different LSTM Epochs (Full)

Due to the long amount of time it takes for 2 epochs to run on the full data set (about 50 minutes per LSTM model), any graph we produce will not have enough data to determine the what number of epochs would be the best fit. So this section will be skipped for the LSTM Full Data and 2 epochs will be used in the experimentation section

### Experimenting with other LSTM models (Full)

To keep the results/comparisons as fair as possible. The LSTM full data will be undergoing the same tests as the LSTM reduced.
"""

def lstmFull2():
# Inform that the model is building
  print('Build model...')

  model = Sequential()

  # Add the embedding layer
  model.add(Embedding(max_features, embedding_size, input_length=maxlen))
  model.add(Dropout(0.25))
  # 1D Convolution Layer with Sigmoid activation
  model.add(Conv1D(filters, kernel_size, padding='valid', activation='sigmoid', strides=1))
  model.add(MaxPooling1D(pool_size=pool_size))

  # Add LSTM Layer
  model.add(LSTM(lstm_output_size))

  # Use the Softplus activiation function
  model.add(Dense(2,activation='softplus'))

  # Using categorical corss entropy and the SGD optimizer
  model.compile(loss = 'categorical_crossentropy', optimizer='SGD',metrics = ['accuracy'])

  # Print a summar of the model
  print(model.summary())

  return model

"""Building the second LSTM Full Model"""

LSTM_Full2 = lstmFull2()

"""Training the second model with 2 Epochs"""

print('Train...')
LSTM_Full2.fit(X_train, Y_train, batch_size=batch_size, epochs=2,shuffle=True, validation_data=(X_test, Y_test))
print('Model evaluation ',LSTM_Full2.evaluate(X_test,Y_test))

"""Printing the results of LSTM Model 2 (Full)"""

score, acc = LSTM_Full2.evaluate(X_test, Y_test, batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)

"""Compared to the first full LSTM Model, Model 2 took an over 10 point decrease in accuracy. While the results aren't as good as the first full model, the changed parameters are much more effective with the full data than the reduced

Making the third LSTM full model
"""

def lstmFull3():
# Inform that the model is building
  print('Build model...')

  model = Sequential()

  # Add the embedding layer
  model.add(Embedding(max_features, embedding_size, input_length=maxlen))
  model.add(Dropout(0.25))
  # 1D Convolution Layer with Linear activation
  model.add(Conv1D(filters, kernel_size, padding='valid', activation='linear', strides=1))
  model.add(MaxPooling1D(pool_size=pool_size))

  # Add LSTM Layer
  model.add(LSTM(lstm_output_size))

  # Use the Softsign activiation function
  model.add(Dense(2,activation='softsign'))

  # Using categorical corss entropy and the Adadelta optimizer
  model.compile(loss = 'categorical_crossentropy', optimizer='Adadelta',metrics = ['accuracy'])

  # Print a summar of the model
  print(model.summary())

  return model

"""Building the model"""

LSTM_Full3 = lstmReduced3()

"""Train the third LSTM full model with 2 epochs"""

print('Train...')
LSTM_Full3.fit(X_train, Y_train, batch_size=batch_size, epochs=2,shuffle=True, validation_data=(X_test, Y_test))
print('Model evaluation ',LSTM_Full3.evaluate(X_test,Y_test))

"""Printing the results of LSTM model 3"""

# Printing test score and accuracy of the third LSTM model
score, acc = LSTM_Full3.evaluate(X_test, Y_test, batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)

"""This accuracy is comically low. So much like the redueced set. The relu activation provides the best results based on our LSTM expeirments

### Setting up the Data For CNN (Full)

Initalizing the variables for the CNN model. Numbers taken from [here](https://keras.io/examples/imdb_cnn/)
"""

# Note some parameters were changed to be more consistent with the LSTM 
# experiements so we could have an accurate comaprison
max_features = 20000
maxlen = 100
batch_size = 30
embedding_dims = 50
filters = 250
kernel_size = 3
hidden_dims = 250
epochs = 2

"""### CNN Model and Results (Full)

Creating the function for the first CNN model. Based off the link above. It is a sequential model with 9 layer, it will run on 2 epochs
"""

def cnnFull1():
  print('Build model...')
  model = Sequential()

  # we start off with an efficient embedding layer which maps
  # our vocab indices into embedding_dims dimensions
  model.add(Embedding(max_features,
                      embedding_dims,
                      input_length=maxlen))
  model.add(Dropout(0.2))

  # we add a Convolution1D, which will learn filters
  # word group filters of size filter_length:
  model.add(Conv1D(filters,
                  kernel_size,
                  padding='valid',
                  activation='relu',
                  strides=1))
  # we use max pooling:
  model.add(GlobalMaxPooling1D())

  # We add a vanilla hidden layer:
  model.add(Dense(hidden_dims))
  model.add(Dropout(0.2))
  model.add(Activation('relu'))

  # We project onto a single unit output layer, and squash it with a sigmoid:
  model.add(Dense(2))
  model.add(Activation('sigmoid'))

  model.compile(loss='categorical_crossentropy',
                optimizer='adam',
                metrics=['accuracy'])
  
  print(model.summary())
  
  return model

"""Building the first full CNN model"""

CNN_Full1 = cnnFull1()

"""Training the model"""

print('Train...')
CNN_Full1.fit(X_train, Y_train, batch_size=batch_size, epochs=2,shuffle=True, validation_data=(X_test, Y_test))
print('Model evaluation ',CNN_Full1.evaluate(X_test,Y_test))

"""Printing the results"""

# Printing the accuracy and test score of the first CNN1 model
score, acc = CNN_Full1.evaluate(X_test, Y_test, batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)

# Getting the classification metrics of CNN1 model 
CNN_Full1_Metrics = CNN_Full1.predict(X_test, verbose = 2, batch_size = batch_size)
print(metrics.classification_report(Y_test[:,1], np.round(CNN_Full1_Metrics[:,1]) ,target_names = ["Negative", "Positive"]))

"""### Experimentning with diffferent CNN Epochs (Full)

Since each Epoch takes about 11 minutes to run on the Full dataset with CNN, we can experiment a little bit. Let's run a 5 epoch model and compare it to the two
"""

# Training with 3 more epochs
print('Train...')
historyCNN_Full1 = CNN_Full1.fit(X_train, Y_train, batch_size=batch_size, epochs=5,shuffle=True, validation_data=(X_test, Y_test))
print('Model evaluation ', CNN1.evaluate(X_test,Y_test))

# Printing test score and accuracy with 5 epochs
score, acc = CNN_Full1.evaluate(X_test, Y_test, batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)

"""Accuracy has gone up a point with the increase in epochs

Calling the plot function
"""

plot_history(historyCNN_Full1)

"""Although this graph isn't detailed as much as the earlier ones, based on where the validation loss begins to rise again, it looks like a good epoch selection for full CNN would be 3. Let's run some experiements with it.

### Experiementing with Other CNN Models (Reduced)

Experiements will be similar to the one in the earlier sections. Three epochs will be used for the other models.
"""

def cnnFull2():
  print('Build model...')
  model = Sequential()

  # we start off with an efficient embedding layer which maps
  # our vocab indices into embedding_dims dimensions
  model.add(Embedding(max_features,
                      embedding_dims,
                      input_length=maxlen))
  model.add(Dropout(0.2))

  # we add a Convolution1D, which will learn filters
  # word group filters of size filter_length:
  model.add(Conv1D(filters,
                  kernel_size,
                  padding='valid',
                  activation='sigmoid',
                  strides=1))
  # we use max pooling:
  model.add(GlobalMaxPooling1D())

  # We add a vanilla hidden layer:
  model.add(Dense(hidden_dims))
  model.add(Dropout(0.2))
  model.add(Activation('sigmoid'))

  # We project onto a single unit output layer, and squash it with a sigmoid:
  model.add(Dense(2))
  model.add(Activation('softplus'))

  model.compile(loss='categorical_crossentropy',
                optimizer='SGD',
                metrics=['accuracy'])
  
  print(model.summary())
  
  return model

"""Building the model"""

CNN_Full2 = cnnFull2()

"""Training the model with 3 epochs"""

print('Train...')
CNN_Full2.fit(X_train, Y_train, batch_size=batch_size, epochs=3,shuffle=True, validation_data=(X_test, Y_test))
print('Model evaluation ',CNN_Full2.evaluate(X_test,Y_test))

"""Printing the results"""

# Printing the accuracy and test score of the second CNN model
score, acc = CNN_Full2.evaluate(X_test, Y_test, batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)

"""Making the third Full CNN model"""

def cnnFull3():
  print('Build model...')
  model = Sequential()

  # we start off with an efficient embedding layer which maps
  # our vocab indices into embedding_dims dimensions
  model.add(Embedding(max_features,
                      embedding_dims,
                      input_length=maxlen))
  model.add(Dropout(0.2))

  # we add a Convolution1D, which will learn filters
  # word group filters of size filter_length:
  model.add(Conv1D(filters,
                  kernel_size,
                  padding='valid',
                  activation='linear',
                  strides=1))
  # we use max pooling:
  model.add(GlobalMaxPooling1D())

  # We add a vanilla hidden layer:
  model.add(Dense(hidden_dims))
  model.add(Dropout(0.2))
  model.add(Activation('linear'))

  # We project onto a single unit output layer, and squash it with a sigmoid:
  model.add(Dense(2))
  model.add(Activation('softsign'))

  model.compile(loss='categorical_crossentropy',
                optimizer='Adadelta',
                metrics=['accuracy'])
  
  print(model.summary())
  
  return model

"""Building the model"""

CNN_Full3 = cnnFull3()

"""Training the model"""

# 3 Epochs Used
print('Train...')
CNN_Full3.fit(X_train, Y_train, batch_size=batch_size, epochs=3,shuffle=True, validation_data=(X_test, Y_test))
print('Model evaluation ',CNN_Full3.evaluate(X_test,Y_test))

"""Printing the results"""

# Printing the accuracy and test score of the second CNN model
score, acc = CNN_Full3.evaluate(X_test, Y_test, batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)

"""Based on all three models of Full CNN, Model 1 Full with 5 epochs returns the best data"""